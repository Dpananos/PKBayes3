---
title: "Paper 3"
author:
  - "A. Demetri Pananos"
  - "Daniel J. Lizotte"
output: pdf_document
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: references.bib  
---


```{r global-options, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = F,
  message = F,
  warning = F,
  fig.height = 4,
  fig.align = 'center',
  dpi = 400,
  cache = F
)
```

```{r libraries}
library(tidyverse)
library(rstanarm)
library(tidybayes)
library(cmdstanr)
library(duckdb)
library(DBI)
source('R/utils.R')

options(readr.show_progress=F,
        readr.num_columns = 0)

theme_set(theme_light())
cmdstanr::register_knitr_engine(override = F)
```


# Introduction

One goal of personalized medicine is optimized dosing of drugs for individuals [-@morse2015personalized].  When considering optimal doses, a thorough understanding pharmacokinetic (what the body does to the drug) and/or pharmacodynamic (what the drug does to the body) effects are crucial.  To this end, models describing the mediation of pharmacokinetic/pharmacodynamic effects via clinical, genetic, and lifestyle factors have an important role in deciding which patients should get what dose and are sometimes published by research teams collaborating with drug manufacturers using data from clinical trials.

Independent investigators can find themselves in a situation in which data collection from a particular population of interest is achievable. If the data come from practice (e.g. a personalized medical clinic), there may be questions about how new variables not previously studied in clinical trials effect the pharmacokientics/pharamcodynamics of a particular drug.  Running large studies in order to examine the effects of these new variables, or discover effects of other variables, may be out of reach due to a variety of constraints.  Consequently, investigators must think about how best to model the pharmacokinetics, for use in decision making *and* exploration, using the data available to them.

Apixaban is one example of such a scenario.  Pharmacokinetic models have been previously published [-@cirincione2018population, -@ueshima2018population] in collaboration with the drug's manufactur using data from clinical trials.  These studies identified age, sex, body weight, renal function,  patient race, and CYP3A4 inhibitors as modulators of apixaban pharmacokinetics [-@cirincione2018population], though according to authors the effects of some of these variables were not large enough to require clinical dose adjustment. However, even after adjusting for the aforementioned factors, concentrations of apixaban in real life applications have been observed to be larger than what was reported in clinical trials [-@sukumar2019apixaban], raising questions as to the optimal dosing of apixaban for patients outside these trials. Additionally, recent research has indicated appropriateness of dose adjustment criteria are unclear [-@vu2021critical], citing there is no reduction in safety in patients above 75 years of age, below 60kg of body weight, and egfr lower than50 mL/min. The uncertainty regarding dosing criteria and additional variability in concentrations in day to day use suggest that, while previously published models may be internally valid, these models may not be representative of all populations in which apixaban is to be applied.  That is to say, the models may lack a degree of external validity, thus supporting the idea that pharmacokientic models may need to be designed for specific populations of interest. When viewed through a Bayesian lense, the previous modelling work can act as an informative prior on various pharmacokinetic/pharmacodynamic measures.  Creatining new models for populations of interest is then more of a "fine tuning" than an alltogether new appraoch.  Pharmacokinetic models for use in a specific population may then have two goals:  to adjust dosing criteria to be specific for a population, and/or to explore how additional variables (for example, concomitant medications) not included in the previous studies effect particular parts of the pharmacokinetics of apixaban.  

This study seeks to demonstrate how investigators can fit similar models to their pharmacokinetic data with the aim of accomplishing the goals of accurate modelling of pharmacokientics and exploration of effects of new varibales.  We use apixaban as a specific example, but our methodology can be generalized to other drugs where pharmacokientics are of interest.  Importantly, we only focus on pharmacokientics since blood plasma levels correlated closely with the pharmacodynamic effect of apixaban [-@byon2019apixaban]. Our approach leverages a Bayesian methodology to building pharmacokinetic models so that we may incorporate prior information from previous studies. Additionally, we describe how investigators can use *all* relevant data available to them to fit these models and make inferences, even if that data come from controlled studies.  Finally, we show how sparsity inducing priors can be applied to new variables in order to explore how those variables may effect apixaban pharmacokinetics, encouraging negligible effect sizes but allowing for large effects to be detected. We present a small simulation study to demonstrate how smallest meaningful effects can be detected through these priors as a function of sample size. Finally, we use an open source Bayesian language to develop our models, making our code freely available.  Previous models are constructed in a proprietary software toolset, which can come at a high cost. Creation of these models in a free tool removes a barrier to research, making these methods more widely available.



# Background

## Apixaban

Our study uses clinical and experimental plasma concentrations from patients who were prescribed apixaban. Apixaban is a direct acting oral anti-coagulant often prescribed for prevention of stroke and systemic embolism in patients with atrial fibrillation (AF) [-@BMSmonograph; -@byon2019apixaban].  Studies as recent as 2019 have reported excess variability in observed apixaban plasma concentrations in patients with AF [-@sukumar2019apixaban]. Since apixaban plasma concentrations correlate closely with anti-coagulation[-@upreti2013effect,-@frost2013safety,-@frost2013apixaban], excess variability in these concentrations may mean increased risk of bleeding. These findings have raised questions towards the optimal dosing of apixaban in older adults with AF encountered outside of clinical trials.

Additional research into determining factors which explain this excess variability beyond known clinical factors [-@gulilat2020drug] has consequently begun.

## Variable Importance & Simulation Study

Existing studies often use variable selection methods (e.g. variants of stepwise selection, including fitting all submodels [*CITE*]) when faced with the determining which are said to effect the phenomenon under study. Many studies have noted that these techniques result in bias away from the null, exaggerated precision, inaccurate or uninterpretable p values due to inability to properly incorporate uncertainty in the selection process, and can fail to select the "true" model with high confidence even when modelling assumptions are consistent with the true data generating process [*CITE MANY PAPERS*].  Hence, even in the best case scenario where the selection procedure identifies the correct variables, the resulting estimates may not be reliable.  Results from simulation studies on selection methods make a convincing argument to avoid selection methods all together.

Selection methods are intended to answer the question "which variables are important in modelling the outcome", and although simulation studies have demonstrated deficiencies with variable selection, they often do not answer the intended question.  From a Bayesian perspective, selection to include a variable in or out of a model defines a sort of prior on the parameter value; there is a strong preference for a null effect estimate unless the data provide sufficient evidence for free estimation of that effect.  Efforts to operationalize this prior structure in terms of Bayesian inference have lead to a wide variety of sparsity inducing priors, which include spike and slab priors [*CITE*], and horseshoe and Finish horseshoe priors [*CITE*]. These approaches admit that while unlikely that the effects of unimportant variables are exactly 0, they may be small enough to be ignorable.  These priors place the majority of their probability mass near 0, encouraging small effects to be estimated as something negligibly small, but allow for large effects to be identified (with perhaps a small amount of bias depending on the prior hyperparameters and prior structure).

Bias towards a null effect  can be acceptable when the goal is exploration and prediction.  The bias can act as a regularization for predictions hence combating overfitting, and can hedge estimates of novel effects when they are exaggerated due to high variance.  From our perspective, discovery is not about getting the estimate right the first time, its about making progress and identifying directions for further investigation.  Bias in the estimates from sparsity inducing priors puts investigators on the right path with the hopes that additional studies will provide more precise and unbiased estimates.

Biasing the effect downward is likely favorable in scenarios where the covariate of interest is only recorded from one dataset available to investigators, much like the scenarios we describe here.  Densely sampled data may come from highly controlled studies, with very explicit inclusion/exclusion criteria.  Unless the covariate is of primary interest in those studies, it may be the case that subjects are highly homogeneous in many respects (e.g. All being healthy young adults, with no concomitant medications).  In these cases, it may be unlikely that the covariate of interest was recorded, if subjects with that covariate were to be included at all.  It is more probable that novel covariates are collected from observational data (e.g. from a personalized medicine clinic which sees patients irregularly).  The observational nature of the data allows a wider collection of patients to be observed, hence making it more likely that the novel covariate is observed in subjects.  However, observational data are limited in so far as there is likely one measurement per subject, likely due to the high patient burden of dense sampling.  This means that, unless the pharmacokientics are very well understood and all relevent covariates are measured, effects of novel covariates may be confounded due to ommited varibale bias.  If the bias is away from the null effect or if the estimate is highly variable due to the covariate not being sufficiently varirable within the sample, regularization via informative priors can combat this. If the bias is towards the null effect, regularization makes this worse (assuming the effect would be detected at all in the absence of regularization and that the bias due to regularization is sufficiently large to hide this effect).  

To this end, we present a simulation study in which we use a sparsity inducing prior to estimate the effect of a concomitant medication on apixaban pharmacokinetics.  In particular, the medication is assumed to inhibit a particular gene important in the elimination of apixaban, making the bioavailability larger.  We place a double exponential (or Laplace) prior on the effect of the concomitant medicaion, as well as a prior on the parameter for the Laplace distirbution.  This is similar to putting a LASSO penalty on the effect as well as a prior on the LASSO penalty strength [*CITE?*].  Although our simulation only has a single variable of interest, many variables can be used with this prior structure.


<!-- $$ p(\beta_F) = \int \mbox{normal}(\beta_k \vert 0, \sigma) \,  \mbox{exponential}\left(\sigma^2 \Bigg\vert \dfrac{1}{2\tau^2} \right) \,d\sigma = \mbox{Laplace}(\beta_k \vert \tau) $$ -->

For our simulation, we generate data from the posterior of a previously fit model [*CITE SECOND PAPER*]. We simulate 10 datasets from a pre-specified number of densely sampled patients (we examine 5, 10, 20, 30, 40, and 50 densely sampled patients) haven taken their first dose of the drug with a pre-specified and fixed effect of a concomitant medication on the bio-availability of the drug.  We assume that investigators can sparsely sample patients more easily, and so we simulate 10x more sparsely sampled patients who have already achieved steady state. We do this so as to more closely resemble real life scenarios in which patients come into a clinic for a plasma measurement having already been on the drug for sometime. We examine effects of 0, 0.125, 0.25, 0.5, 1.0, and 1.5 on the logit scale (we use the logit scale since bioavailability is constrained to be between 0 and 1). 

## "You May Think Method X might work, and you'd be wrong"

In this paper, we propose a pooling of both sparsely and densely sampled data in a single model. Pooling information is not a new approach, and reasonable arguments could be made to use simpler models.  After all, if the sparsely sampled data models a continuous outcome as a function of covariates, why would investigators use a complex model when something simple like linear regression (or linear regression on log concentrations) may be sufficient.  While simpler approaches and criticisms of using unnesecarily complex models are valid, both linear modelling and mixed effects models for pooling suffer from important drawbacks in the case when attempting to combine sparsely sampled and densley sampled data from different studies.  We examine those drawbacks below.

Linear regression can be, and has been [*CITE*], used to model apixaban concentrations as a function of time and other covariates using sparsely sampled data.  When certain criteria are met, there is good reason to do so.  The concentration profile, $y(t)$, from a first order absorption with linear elimination pharmacokinetic model looks like

$$ y(t) = \frac{F \cdot D}{C l} \frac{k_{e} \cdot k_{a}}{k_{e}-k_{a}}\left(e^{-k_{a}t}-e^{-k_{e}t}\right) $$
The elimination phase occurs when $t$ is sufficiently large, resulting in $y(t)$ being approximately exponential and $\log(y(t))$ being linear in time with slope $-k_e$.  Assuming measurement error is additive on the log scale facilitates use of linear regression.

This approach is common in pharmacokinetics when estimating the elimination rate but suffers from three important drawbacks generally. First, the elimination rate is now allowed to vary as a function of known factors which effect elimination rate, such as kidney function.  This can be ameliorated by specifying an interaction between time and those covariates known to effect elimination rate (though this has not been done in all papers [*CITE*]).  Second, an exponential approximation is only appropriate when time is sufficiently large.  Clearly, the exponential approximation breaks down near $t=t_{max} = \log(k_a/k_e)/(k_a-k_e)$ and is completely inappropriate in the absorption phase when $\partial y(t) / \partial t >0$.  This effects estimates of max concentration in an appreciable way, resulting in an upward bias of $C_max$ (the bias is equivalent to $-k_a t$).  Additionally, because $t_{max}$ is not modeled per individual, estimates of $C_{max}$ must rely on a point estimate of $t_{max}$.  This results in uncertainty estimates of $C_{max}$ which may be too narrow for a given individual.  Finally, the effects of covariates on other aspects of the pharmacokinetcs are undetermined. Assuming a linear model is used to model concentrations on the log scale, we find

$$ \log(y(t)) \approx \log(D) + \log(F) - \log(Cl) + \log(k_e) + \log(k_a) - \log(k_e-k_a) - k_et = \log(D) + \beta_0 + \beta_1t \>. $$
Here, $\beta_0 =  \log(F) - \log(Cl) + \log(k_e) + \log(k_a) - \log(k_e-k_a)$.  If covarates are included in the model, then althrough changes in log concentration may be accurate (in so far as the magnitude of the increase or decrease in log concentration is concerned), *where that change occurs is underdetermined*.  Did concentration increase because bioavailability ($\log(F)$ in the log linear model) increased, or was it because the clearance rate ($\log(Cl)$ in the log linear model) decreased?  We can't say for certain from this model. In order to determine if a change in concentration was due to an increase/decrease in a pharmacokinetic parameter, each pharamacokinetic parameter must be modelled as functions of covariates. How salient these drawbacks are is up to the investigator, but if any of them are important to deicsion making for personalized medicine then a linear model may not be appropriate.

Mixed effects models can be used to pool information from many datasets.  Meta-analysis is perhaps the most prevalent example of this approach.  A typical example may be pooling data from studies conducted using similar protocols across multiple centers.  Ideally, the data are collected under similar protocols, making assumptions regarding the likelihood and exchangeability of appropriate units tenable.  In the scenario we describe, where information from at least two studies with different protocols are to be pooled, we believe a mixed effect model specifying between study variation is *not* appropriate due to subjects not being exchangeable between studies.  Recall, a sequence of random variables $\theta_1, \dots, \theta_n$ is said to be exchangeable in their joint density if $p(\theta_1, \dots, \theta_n)$ is invariant permutations of the indicies $(1, \dots, n)$ [*CITE BDA3*].  If no other information, other than observed data, is available to distiguish any of the $\theta_j$ from any others, and no ordering or grouping of parameters can be made, one must assume exchageability of the $\theta$.  In the scenarios we describe, we do have additional information which can be used to distinguish the $\theta$.  Assuming here $\theta$ represents individuals, qualitative information on how the study was conducted (their inclusion/exclusion criteria, how controlled the study was, etc.) acts as a way of distinguishing between the $\theta$ which is not easily incorporated into the data.  Considering these differences, pooling the data from two studies which have not followed the same protocol violates exchageability, preventing use of heirarchial modelling as the method of pooling.  Subjects *within* each study can be considered exchangeable, and so a heriarchal model can be used to model data from each study, though it can not be used to pool information between studies.


# Methods

By now, we have established a few points which are worth reiterating:

* In cases where specific populations of interest display sufficient differences as compared to data presented in clinical trials, the tailoring of a pharmacokinetic model to the intended population for use in personalized medicine applications may be desirable.  Apixaban is one such drug which displays such differences, and we use data from apixaban as a case study.


* Models developed for specific populations may serve two purposes: prediction and exploration of novel effects.  Previous studies rely on variable selection procedures to identify novel predictors of pharmacokientcs.  As of late, many simulation studies have demonstrated defficiencies in variable selection procedures.  We propose the use of sparsity inducing priors to regularize negligble effects towards 0 while keeping those variables in the models.

* Investigators may want to make use of all pharmacokinetic data available to them, be they sparsley sampled and of an observational nature, or densley sampled from well controled clinical studies.  Typical methods for pooling this information (e.g. mixed effect models, or simply concatinating datasets) are not unversally appropriate in cases when data come from studies with vastly different protocols, thereby violating exchangeability.

In what follows, we present a Bayesian model which adresses all three points above.  We present a model of apixaban pharmacokinetics which combines data from two studies with different protocols.  We demonstrate how sparse priors can be used to estimate the effect of a potentially novel predictor, and present a simulation study to investigate how relative sample sizes between the two studies and effect size of the predictor effect estimation.

## Bayesian Model

Our model specifies a population level effect of covariates (age, sex, weight (kg), serum creatinine $\mu \mbox{mol}$) on patient clearance, time to max concentration, and the ratio between absorption and elimination rates (a unitless parameter we refer to as $\alpha$). These effects are shared between all populations, allowing information from one dataset to partially inform model fit on the other.  We also include a population level effect of concomitant amidarone on bioavailability of apixaban.  

We fit our model using Stan [-@gelman2015stan], an open source probabilistic programming language with interfaces to Python, R, Stata, Matlab, and more.  Fittting two datasets jointly is formally equivalent to fitting one dataset first and passing the posterior as a prior for a model for the other dataset.  However, such an approach requires summarization of the posterior, which may result in loss of information (such as covariance between draws, unless explicitly modeled).  The recommended approach is then to fit datasets jointly, as done in [-@fallingBetancourt].



### Dense Sampling (Dataset 1) Model

Since patients are observed multiple times in these data, this offers the opportunity to estimate random effects for Clearence $Cl$, time to mac concentration $t_{\max}$, ratio between elimination and absorption rates $\alpha = k_e/k_a$.

Let $X$ be a matrix of mean centered and standardized covariates for dataset 1.  For patient $j$, we model the pharmacokientic parameters as

$$ \log(Cl_{j}) = \mu_{Cl} + X\beta_{Cl} + z_{Cl, j} \sigma_{Cl}  $$
$$ \log(t_{\max,j}) = \mu_{t_{\max}} + X\beta_{t_{max}} + z_{t_{\max, j}}\sigma_{t_{\max}}  $$

$$ \operatorname{logit}(\alpha_{j})  =  \mu_{\alpha} + X\beta_{\alpha} + z_{\alpha, j}\sigma_{\alpha}  $$

Here, the $\mu$ are the population level means for the indicated pharmacokinetic parameters, the $\beta$ are the regression coefficients, the $z$ are standard normal random variables to account for random effects, and the $\sigma$ are the standard deviations of the population distribution for the indicated pharmacokinetic parameters. Additionally, we model the population mean for the bioavailability as $F = 1/(1 + e^{\mu_F}$, with a prior on $\mu_F$.  Both the $\mu$ and the $\beta$ are shared between datasets.

For dataset 1, we also model a delay between ingestion and absorption of apixaban.  The delay is modeled as 

$$ \delta_j = 0.5 \times b  $$

Where $b$ is a beta distributed random variable with parameters learned from the data.  The factor of 0.5 is used to ensure that at $t=0.5$ hours after ingestion, the predicted to be non-zero.

We use a one compartment pharmacokinetic model with first order elimination as our conditional mean

$$  C_{j}(t)= \begin{cases}\frac{F \cdot D}{C l_j} \frac{k_{e, j} \cdot k_{a, j}}{k_{e, j}-k_{a, j}}\left(e^{-k_{a, j}(t-\delta_j)}-e^{-k_{e, j}(t-\delta_j)}\right) & \delta_j \leq t \\ 0 & \text { else }\end{cases} \>. $$
Here, all parameters are estimated from data, and we have used the facts that

$$ t_{\max }=\frac{\ln \left(k_{a}\right)-\ln \left(k_{e}\right)}{k_{a}-k_{e}} $$
$$ \alpha = \dfrac{k_e}{k_a} $$

in order to solve for $k_e$ and $k_a$ for use in our PK model.  Finally, we specify a lognormal likelihood for dataset 1

$$ y_j \sim \mbox{Lognormal}(C_j(t), \sigma_1) \>. $$

For information of prior distributions, see our supplement.

### Real Life Data (Dataset 2) Model

Much of the structure from the previous model is translated to dataset 2.  However, there are a few differences.

Because patients in this dataset are not measured multiple times we do not estimate random effects or a time delay.  Hence, we model

$$ \log(Cl) = \mu_{Cl} + X\beta_{Cl} $$

$$ \log(t_{\max}) = \mu_{t_{\max}} + X\beta_{t_{\max}} $$

$$\operatorname{logit}(a) = \mu_\alpha + X\beta_{\alpha} $$

Where the $mu$ and the $\beta$ are shared between datasets.  Additionally, we model the bioavailbility as 

$$ \operatorname{logit}(F) = \mu_F   + \beta_{amio} \mbox{amiodarone}$$

as dataset 2 has information regarding concomitant amiodarone.  The prior for the effect of concomitant amiodarone is a sparsity inducing prior, meaning it encourages negligble effects (near 0) but will allow for large effects to be identified.

Finally, data from dataset 2 comes from patients who have been taking apixaban twice daily.  Hence, their initial plasma concentration on ingestion is not 0, but can be modeled using the pharmacokientics none the less.  Assuming the patients have been taking apixaban twice a day, 12 hours apart, for the last 5 days, the initial concentration can be shown to be

$$ C_0 = \sum_{j=1}^{10} C(12j) \>. $$

Here, $C(t)$ is the pharmacokintic profile with estimated coefficients for that patient.  See our supplement for a proof of this proposition.

Our pharmacokientic profile is again provided by a one compartment first order elimination 

$$  C(t)= c_0 + \frac{F \cdot D}{C l} \frac{k_{e} \cdot k_{a}}{k_{e}-k_{a}}\left(e^{-k_{a}(t)}-e^{-k_{e}(t)}\right)  $$
and we assume a lognormal likelihood

$$ y_j \sim \mbox{Lognormal}(C(t), \sigma_2) \>. $$
Note the likelihood for dataset 2 has a different observational noise component ($\sigma_2$) as compared to dataset 1 ($\sigma_1$).  This is because random effects can not be estimated from dataset 2, hence the residual variance is part observational noise and part between subject variability conditional on the subject covariates.

# Results

The results from our simulation study are shown below. The precision of the estimate of effect of concimitant drug use increases as the number of densley sampled (and sparsely sampled) patientcs increases.  Show in read are the sample means of the 10 runs (black dots).  On average we see a small amount of bias in the estimates.  This is expected since the sparsity inducing priors have the majority of their density in a small neighbourhood of 0, regularizing effects towards 0.  For purposes of discovery, these biases may be acceptable.


When using real data, our model can accurately predict both densley sampled and sparsley sampled data.  Shown in figure x is a log-log plot of predicted and actual concentrations for both datasets.  The model makes more accurate predictions for densley sampled patients (because it is able to estimate the random effect in each pharmacokinetic parameter).  The apparent increase in prediction error for the sparsley sampled can be explained by the absence of random effects for each patient.  The within and between patient variation manfiests as measurement error solely, thus leading to lower predictive ability.  This perspective is suppported when examining the measurement error posterior distirbutions for both dense/sparse patients.

With a model for the pharmacokinetics of apixaban in hand, estimates of salient pharmacokientic phenomena can be easily obtained.  In figure y, we use our model to estimate the max concentration for the reference patient under different doses of amiodarone.  Through our model, we estimate concomitant amiodarone increases bioavailability, which in turn increases max concentration.  Shown in black is the expected max concentration conditioned on concomitant amiodarone dose, as well as 95% equal tailed posterior credible intervals.

```{r}
sim_results<-list.files('python/simulation_data/', full.names = T) %>% 
             map_dfr(read_csv)

sim_results %>%
  rename(estimate=mean) %>%
  ggplot(aes(amio_effect, estimate))+
  geom_abline(color = 'dark grey')+
  geom_point(alpha = 0.5)+
  # geom_pointrange(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5, fill = 'gray', color = 'black', position = position_dodge2(width = 0.25))+
  stat_summary(color='red', geom='line')+
  facet_wrap(~num_dense_patients)+
  labs(x='True Bioavailability', y = 'Error')


# sim_results %>% 
#   rename(estimate=mean) %>% 
#   ggplot(aes(factor(amio_effect), estimate))+
#   geom_pointrange(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5, fill = 'gray', color = 'black', position = position_dodge2(width = 0.5))+
#   geom_point(aes(factor(amio_effect), amio_effect), color = 'red')+
#   facet_wrap(~num_dense_patients)
```


```{r load-in-data}
con = dbConnect(duckdb(),'data/database/apixaban_data.duckdb')

ute_data = tbl(con, 'ute_cleaned_data') %>% 
           collect() %>% 
           mutate(
             dataset = "Ute's Data",
             i = seq_along(subjectids)
           ) 

rommel_data = tbl(con, 'rommel_cleaned_data') %>% 
              collect() %>% 
              mutate(
                subjectids = as.character(subjectids),
                dataset = "Rommel's Data"
                )

dbDisconnect(con, shutdown=T)


```

```{r load-in-model}
fit = readRDS('model_008.RDS')

conc = function(time, d, f,cl, ke, ka, c0){
  d*f*ke*ka/(cl*(ke-ka)) * (exp(-ka*time) - exp(-ke*time)) + c0
}

```


```{r plot-model-predictions}

# Check for data from one of those data.
# Public apixaban data?
rommel_fit = fit %>% 
  spread_draws(r_C[i]) %>% 
  mutate(r_C = 1000*r_C) %>% 
  mean_qi() %>% 
  bind_cols(rommel_data) %>% 
  rename(predictions=r_C)


ute_fit = fit %>% 
  spread_draws(u_C[i]) %>% 
  mutate(u_C = 1000*u_C) %>% 
  mean_qi() %>% 
  bind_cols(ute_data) %>% 
  rename(predictions=u_C)

rommel_fit %>% 
  bind_rows(ute_fit) %>% 
  ggplot(aes(log(yobs_ng_ml), log(predictions), ymin = log(.lower), ymax = log(.upper)))+
  geom_pointrange(size=0.1)+
  geom_abline(color='dark grey')+
  facet_wrap(~dataset, scale='free')+
  theme(aspect.ratio = 0.62)+
  labs(x='Log Concentration', y='Log Prediction')

ggsave('prediction_v_actual.png', path='figures')
knitr::plot_crop('figures/prediction_v_actual.png')

```

```{r}

fit %>% 
  spread_draws(mu_cl, s_cl, mu_tmax, s_t, mu_alpha, s_alpha, mu_F, b_F_amio) %>% 
  mutate(
    cl = exp(mu_cl),
    tmax = exp(mu_tmax),
    a = plogis(mu_alpha),
    ka = log(a)/(tmax*(a-1)),
    ke = ka*a
  ) %>% 
  select(cl, tmax, ka, ke, mu_F, b_F_amio) %>%  
  crossing(amio = seq(0, 1, 0.05)) %>% 
  mutate(f = plogis(mu_F + b_F_amio*amio),
         y = conc(tmax, 5, f, cl, ke, ka, 0)) %>% 
  ggplot(aes(amio, y))+
  stat_lineribbon(.width = c(0.5, 0.8, 0.95), size=1)+
  scale_fill_brewer(palette ='YlGnBu')+
  scale_x_continuous(labels = function(x) 400*x)

```

# Discussion

In cases like apixaban, where real world data indicates previously published models may underestimate some aspect of pharmacokientics important to personalizing doses, investigators may consider constructing their own model for use in personalization as well as in exploration for novel effects.  In such cases making use of all available data, be the data from densely sampled highly controlled studies or routinely collected samples from a personalized medicine clinic, is vital when running a large scale study is unfeasible.  In this work, we have shown how data from the same population but different studies can be combined in the same model to increase the precision of estimates for pharmcokinetic effects. Our model posits that the data share something in common (in our case, the effects of the covariates are common between the two studies) and allows for the observational model to be different where needed.  In our example, our densley sampled data allowed for a time delay to be estimated from the data (essentially the time between ingestion of the bolus dose and absorption into the blood stream), as well as random effects.  Additionally, although the two datasets used the same measurement process, the sparse data appeared to have larger measurement error due to within and between subject variation. Extensions to include more datasets are straightforward, and we present a sort of minimal example with two different datasets.  Pooling data from separate but similar datasets is not a new approach.  Mixed effects models are typically used to pool information from different datasets, perhaps by specifying a study specific random effect.  However, this requires that the design of the studies be similar in order to facilitate the pooling.  The best example is a multi-center study where a similar protocol is being used to measure and collect data.  Such an approach is not appropriate for all circumstances since data on the same phenomenon can be collected in different ways.  Our approach is intended to be used in scenarios where mixed effects models can not be simply applied, and where the data generating process may require bespoke modelling to satisfy the unique conditions under which the data were collected.

In addition to tailoring the model to the population of interest, investigators may also be interested in examining effects of covariates not previously studied (e.g. concomitant medications) in clinical trials.  Where as previous studies use variable selection, we propose using sparse priors to detect large effects at the cost of bias in those effects towards the null.  In our simulation, we are able to detect both small and large at the expense of bias in the estimate.  This bias attenuates as sample size increases, as expected.  From the perspective of exploration, the bias is acceptable for two reasons.  First, no single study is intended to be an *experimentum crucis*.  Exploratory studies are, from our perspective, intended to generate hypotheses to be further examined in subsequent studies and populations. In these cases, the more salient factors are the magnitude of the estimate (is the effect "big" or "small", for some definition of those) and the sign of the effect (does the new covariate increase or decrease some quantity).  A Bayesian approach with sparsity inducing priors allows for the type S (S for sign) and type M (M for magnitude) error rates to be controlled for, as is demonstrated in [*CITE GELMAN*]. Our simulation focused on a sparse prior for a single covariate, but the approach can easily be generalized to many covariates.  Similar approaches have been applied in GWAS literature, where many thousands of candidate genes are under consideraiton.  In principle, sparsity inducing priors could be used to select genes within the pharmacokinetic model, combining PK modelling with GWAS style analyses.







# References