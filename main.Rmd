---
title: "Paper 3"
author:
  - "A. Demetri Pananos"
  - "Daniel J. Lizotte"
output: pdf_document
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: references.bib  
---


```{r global-options, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = F,
  message = F,
  warning = F,
  fig.height = 4,
  fig.align = 'center',
  dpi = 400,
  cache = F
)
```

```{r libraries}
library(tidyverse)
library(rstanarm)
library(tidybayes)
library(cmdstanr)
library(duckdb)
library(DBI)
source('R/utils.R')

theme_set(theme_light())
cmdstanr::register_knitr_engine(override = F)
```


# Introduction

One goal of personalized medicine is optimized dosing of drugs for individuals [-@morse2015personalized].  When considering optimal doses, a thorough understanding pharmacokinetic (what the body does to the drug) and/or pharmacodynamic (what the drug does to the body) effects are crucial.  To this end, models describing the mediation of pharmacokinetic/pharmacodynamic effects via clinical, genetic, and lifestyle factors have an important role in deciding which patients should get what dose and are sometimes published by research teams collaborating with drug manufacturers using data from clinical trials.

Independent investigators can find themselves in a situation in which data collection from a particular population of interest is achievable. If the data come from practice (e.g. a personalized medical clinic), there may be questions about how new variables not previously studied in clinical trials effect the pharmacokientics/pharamcodynamics of a particular drug.  Running large studies in order to examine the effects of these new variables, or discover effects of other variables, may be out of reach due to a variety of constraints.  Consequently, investigators must think about how best to model the pharmacokinetics, for use in decision making *and* exploration, using the data available to them.

Apixaban is one example of such a scenario.  Pharmacokinetic models have been previously published [-@cirincione2018population, -@ueshima2018population] in collaboration with the drug's manufactur using data from clinical trials.  These studies identified age, sex, body weight, renal function,  patient race, and CYP3A4 inhibitors as modulators of apixaban pharmacokinetics [-@cirincione2018population], though according to authors the effects of some of these variables were not large enough to require clinical dose adjustment. However, even after adjusting for the aforementioned factors, concentrations of apixaban in real life applications have been observed to be larger than what was reported in clinical trials [-@sukumar2019apixaban], raising questions as to the optimal dosing of apixaban for patients outside these trials. Additionally, recent research has indicated appropriateness of dose adjustment criteria are unclear [-@vu2021critical], citing there is no reduction in safety in patients above 75 years of age, below 60kg of body weight, and egfr lower than50 mL/min. The uncertainty regarding dosing criteria and additional variability in concentrations in day to day use suggest that, while previously published models may be internally valid, these models may not be representative of all populations in which apixaban is to be applied.  That is to say, the models may lack a degree of external validity, thus supporting the idea that pharmacokientic models may need to be designed for specific populations of interest.  Creating similar pharmacokinetic models for use in a specific population may have two goals:  to adjust dosing criteria to be specific for a population, and/or to explore how additional variables (for example, concomitant medications) not included in the previous studies effect particular parts of the pharmacokinetics of apixaban.  

This study seeks to demonstrate how investigators can fit similar models to their pharmacokinetic data with the aim of accomplishing the goals of accurate modelling of pharmacokientics and exploration of effects of new varibales.  We use apixaban as a specific example, but our methodology can be generalized to other drugs where pharmacokientics are of interest.  Additioanlly, we focus only on pharmacokientics since blood plasma levels correlated closely with the pharmacodynamic effect of apixaban [-@byon2019apixaban]. Our approach leverages a Bayesian methodology to building pharmacokinetic models so that we may incorporate prior information from previous studies. Additionally, we describe how investigators can use *all* relevant data available to them to fit these models and make inferences, even if that data come from controlled studies.  Finally, we show how sparsity inducing priors can be applied to new variables in order to explore how those variables may effect apixaban pharmacokinetics, encouraging small effects but allowing for large effects to be detected. We present a small simulation study to demonstrate how smallest meaningful effects can be detected through these priors as a function of sample size. Finally, we use an open source Bayesian language to develop our models, making our code freely available.  Previous models are constructed in a proprietary software toolset, which can come at a high cost. Creation of these models in a free tool removes a barrier to research, making these methods more widely available.



# Background

## Apixaban

Our study uses clinical and experimental plasma concentrations from patients who were prescribed apixaban. Apixaban is a direct acting oral anti-coagulant often prescribed for prevention of stroke and systemic embolism in patients with atrial fibrillation (AF) [-@BMSmonograph; -@byon2019apixaban].  Studies as recent as 2019 have reported excess variability in observed apixaban plasma concentrations in patients with AF [-@sukumar2019apixaban]. Since apixaban plasma concentrations correlate closely with anti-coagulation[-@upreti2013effect,-@frost2013safety,-@frost2013apixaban], excess variability in these concentrations may mean increased risk of bleeding. These findings have raised questions towards the optimal dosing of apixaban in older adults with AF encountered outside of clinical trials.

Additional research into determining factors which explain this excess variability beyond known clinical factors [-@gulilat2020drug] has consequently begun.

## Methods


# Variable Importance & Simulation Study

Existing studies often use variable selection methods (e.g. variants of stepwise selection, including fitting all submodels [*CITE*]) when faced with the determining which are said to effect the phenomenon under study. Many studies have noted that these techniques result in bias away from the null, exaggerated precision, inaccurate or uninterpretable p values due to inability to properly incorporate uncertainty in the selection process, and under mild assumptions can fail to select the "true" model with high confidence [*CITE MANY PAPERS*].  Results from simulation studies on selection methods make a convincing argument to avoid selection methods. 

Selection methods are intended to answer the question "which variables are important", and although simulation studies have demonstrated deficiencies with variable selection, they often do not answer the intended question.  From a Bayesian perspective, selection to include a variable in or out of a model defines a sort of prior on the parameter value; there is a strong preference for no effect unless the data provide sufficient evidence for free estimation of that effect.  Efforts to operationalize this prior structure in terms of Bayesian inference have lead to a wide variety of sparsity inducing priors, which include: spike and slab priors [*CITE*], and horseshoe and finish horseshoe priors [*CITE*]. These approaches admit that while unlikely that effects of variables are exactly 0, they may be too small to care about.  These priors place the majority of their probability mass near 0, enouraging small effects to be estimated as something negligibly small, but allow for large effects to be identified (with perhaps a small amount of bias depending on the prior hyperparameters and prior structure).

Bias towards a null effect  can be acceptable when the goal is exploration and prediction.  The bias can act as a regularization for predictions hence combating overfitting, and can hedge estimates of novel effects when they are exagerated due to high variance.  In our perspective, discovery is not about getting the estimate right the first time, its about making progress and identifying directions for further investigation.  Bias in the estimates from sparsity inducing priors achieves puts investigators on that path.

To this end, we present a simulation study in which we use a sparsity inducing prior to estimate the effect of a concomitant medication on apixaban pharmacokinetics.  In particular, the medication is assumed to inhibit a particular gene important in the elimination of apixaban, making the bioavailability larger.  We place a double exponential (or Laplace) prior on the effect of the concomitant medicaion, as well as a prior on the parameter for the Laplace distirbution.  This is similar to putting a LASSO penalty on the effect as well as a prior on the LASSO penalty strength [*CITE?*].  Although our simulation only has a single variable of interest, many variables can be used with this prior structure.


<!-- $$ p(\beta_F) = \int \mbox{normal}(\beta_k \vert 0, \sigma) \,  \mbox{exponential}\left(\sigma^2 \Bigg\vert \dfrac{1}{2\tau^2} \right) \,d\sigma = \mbox{Laplace}(\beta_k \vert \tau) $$ -->

For our simulation, we generate data from the posterior of a previously fit model [*CITE SECOND PAPER*]. We simulate 10 datasets from a pre-specified number of densley sampled patients (we examine 5, 10, 20, 30, 40, and 50 densely sampled patients) haven taken their first dose of the drug with a pre-specified and fixed effect of a concomitant medication on the bio-availability of the drug.  We assume that investigators can sparsely sample patients more easily, and so we simulate 10x more sparsley sampled patients who have already achieved steady state. We do this so as to more closely resemble real life scenarios in which patients come into a clinic for a plasma measurmeent having already been on the drug for sometime. We examine effects of 0, 0.125, 0.25, 0.5, 1.0, and 1.5 on the logit scale (we use the logit scale since bioavailability is constrained to be between 0 and 1). 


## Bayesian Model

Our model specifies a population level effect of covariates (age, sex, weight (kg), serum creatinine $\mu \mbox{mol}$) on patient clearance, time to max concentration, and the ratio between absorption and elimination rates (a unitless parameter we refer to as $\alpha$). These effects are shared between all populations, allowing information from one dataset to partially inform model fit on the other.  We also include a population level effect of concomitant amidarone on bioavailability of apixaban.  

We fit our model using Stan [-@gelman2015stan], an open source probabilistic programming language with interfaces to Python, R, Stata, Matlab, and more.  Fittting two datasets jointly is formally equivalent to fitting one dataset first and passing the posterior as a prior for a model for the other dataset.  However, such an approach requires summarization of the posterior, which may result in loss of information (such as covariance between draws, unless explicitly modeled).  The recommended approach is then to fit datasets jointly, as done in [-@fallingBetancourt].



### Dense Sampling (Dataset 1) Model

Since patients are observed multiple times in these data, this offers the opportunity to estimate random effects for Clearence $Cl$, time to mac concentration $t_{\max}$, ratio between elimination and absorption rates $\alpha = k_e/k_a$.

Let $X$ be a matrix of mean centered and standardized covariates for dataset 1.  For patient $j$, we model the pharmacokientic parameters as

$$ \log(Cl_{j}) = \mu_{Cl} + X\beta_{Cl} + z_{Cl, j} \sigma_{Cl}  $$
$$ \log(t_{\max,j}) = \mu_{t_{\max}} + X\beta_{t_{max}} + z_{t_{\max, j}}\sigma_{t_{\max}}  $$

$$ \operatorname{logit}(\alpha_{j})  =  \mu_{\alpha} + X\beta_{\alpha} + z_{\alpha, j}\sigma_{\alpha}  $$

Here, the $\mu$ are the population level means for the indicated pharmacokinetic parameters, the $\beta$ are the regression coefficients, the $z$ are standard normal random variables to account for random effects, and the $\sigma$ are the standard deviations of the population distribution for the indicated pharmacokinetic parameters. Additionally, we model the population mean for the bioavailability as $F = 1/(1 + e^{\mu_F}$, with a prior on $\mu_F$.  Both the $\mu$ and the $\beta$ are shared between datasets.

For dataset 1, we also model a delay between ingestion and absorption of apixaban.  The delay is modeled as 

$$ \delta_j = 0.5 \times b  $$

Where $b$ is a beta distributed random variable with parameters learned from the data.  The factor of 0.5 is used to ensure that at $t=0.5$ hours after ingestion, the predicted to be non-zero.

We use a one compartment pharmacokinetic model with first order elimination as our conditional mean

$$  C_{j}(t)= \begin{cases}\frac{F \cdot D}{C l_j} \frac{k_{e, j} \cdot k_{a, j}}{k_{e, j}-k_{a, j}}\left(e^{-k_{a, j}(t-\delta_j)}-e^{-k_{e, j}(t-\delta_j)}\right) & \delta_j \leq t \\ 0 & \text { else }\end{cases} \>. $$
Here, all parameters are estimated from data, and we have used the facts that

$$ t_{\max }=\frac{\ln \left(k_{a}\right)-\ln \left(k_{e}\right)}{k_{a}-k_{e}} $$
$$ \alpha = \dfrac{k_e}{k_a} $$

in order to solve for $k_e$ and $k_a$ for use in our PK model.  Finally, we specify a lognormal likelihood for dataset 1

$$ y_j \sim \mbox{Lognormal}(C_j(t), \sigma_1) \>. $$

For information of prior distributions, see our supplement.

### Real Life Data (Dataset 2) Model

Much of the structure from the previous model is translated to dataset 2.  However, there are a few differences.

Because patients in this dataset are not measured multiple times we do not estimate random effects or a time delay.  Hence, we model

$$ \log(Cl) = \mu_{Cl} + X\beta_{Cl} $$

$$ \log(t_{\max}) = \mu_{t_{\max}} + X\beta_{t_{\max}} $$

$$\operatorname{logit}(a) = \mu_\alpha + X\beta_{\alpha} $$

Where the $mu$ and the $\beta$ are shared between datasets.  Additionally, we model the bioavailbility as 

$$ \operatorname{logit}(F) = \mu_F   + \beta_{amio} \mbox{amiodarone}$$

as dataset 2 has information regarding concomitant amiodarone.  The prior for the effect of concomitant amiodarone is a sparsity inducing prior, meaning it encourages negligble effects (near 0) but will allow for large effects to be identified.

Finally, data from dataset 2 comes from patients who have been taking apixaban twice daily.  Hence, their initial plasma concentration on ingestion is not 0, but can be modeled using the pharmacokientics none the less.  Assuming the patients have been taking apixaban twice a day, 12 hours apart, for the last 5 days, the initial concentration can be shown to be

$$ C_0 = \sum_{j=1}^{10} C(12j) \>. $$

Here, $C(t)$ is the pharmacokintic profile with estimated coefficients for that patient.  See our supplement for a proof of this proposition.

Our pharmacokientic profile is again provided by a one compartment first order elimination 

$$  C(t)= c_0 + \frac{F \cdot D}{C l} \frac{k_{e} \cdot k_{a}}{k_{e}-k_{a}}\left(e^{-k_{a}(t)}-e^{-k_{e}(t)}\right)  $$
and we assume a lognormal likelihood

$$ y_j \sim \mbox{Lognormal}(C(t), \sigma_2) \>. $$
Note the likelihood for dataset 2 has a different observational noise component ($\sigma_2$) as compared to dataset 1 ($\sigma_1$).  This is because random effects can not be estimated from dataset 2, hence the residual variance is part observational noise and part between subject variability conditional on the subject covariates.

# Results

The results from our simulation study are shown below. The precision of the estimate of effect of concimitant drug use increases as the number of densley sampled (and sparsely sampled) patientcs increases.  Show in read are the sample means of the 10 runs (black dots).  On average we see a small amount of bias in the estimates.  This is expected since the sparsity inducing priors have the majority of their density in a small neighbourhood of 0, regularizing effects towards 0.  For purposes of discovery, these biases may be acceptable.


When using real data, our model can accurately predict both densley sampled and sparsley sampled data.  Shown in figure x is a log-log plot of predicted and actual concentrations for both datasets.  The model makes more accurate predictions for densley sampled patients (because it is able to estimate the random effect in each pharmacokinetic parameter).  The apparent increase in prediction error for the sparsley sampled can be explained by the absence of random effects for each patient.  The within and between patient variation manfiests as measurement error solely, thus leading to lower predictive ability.  This perspective is suppported when examining the measurement error posterior distirbutions for both dense/sparse patients.

With a model for the pharmacokinetics of apixaban in hand, estimates of salient pharmacokientic phenomena can be easily obtained.  In figure y, we use our model to estimate the max concentration for the reference patient under different doses of amiodarone.  Through our model, we estimate concomitant amiodarone increases bioavailability, which in turn increases max concentration.  Shown in black is the expected max concentration conditioned on concomitant amiodarone dose, as well as 95% equal tailed posterior credible intervals.

```{r}
sim_results<-list.files('python/simulation_data/', full.names = T) %>% 
             map_dfr(read_csv)

sim_results %>%
  rename(estimate=mean) %>%
  ggplot(aes(amio_effect, estimate))+
  geom_abline(color = 'dark grey')+
  geom_point(alpha = 0.5)+
  # geom_pointrange(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5, fill = 'gray', color = 'black', position = position_dodge2(width = 0.25))+
  stat_summary(color='red', geom='line')+
  facet_wrap(~num_dense_patients)+
  labs(x='True Bioavailability', y = 'Error')


# sim_results %>% 
#   rename(estimate=mean) %>% 
#   ggplot(aes(factor(amio_effect), estimate))+
#   geom_pointrange(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5, fill = 'gray', color = 'black', position = position_dodge2(width = 0.5))+
#   geom_point(aes(factor(amio_effect), amio_effect), color = 'red')+
#   facet_wrap(~num_dense_patients)
```


```{r load-in-data}
con = dbConnect(duckdb(),'data/database/apixaban_data.duckdb')

ute_data = tbl(con, 'ute_cleaned_data') %>% 
           collect() %>% 
           mutate(
             dataset = "Ute's Data",
             i = seq_along(subjectids)
           ) 

rommel_data = tbl(con, 'rommel_cleaned_data') %>% 
              collect() %>% 
              mutate(
                subjectids = as.character(subjectids),
                dataset = "Rommel's Data"
                )

dbDisconnect(con, shutdown=T)


```

```{r load-in-model}
fit = readRDS('model_008.RDS')

conc = function(time, d, f,cl, ke, ka, c0){
  d*f*ke*ka/(cl*(ke-ka)) * (exp(-ka*time) - exp(-ke*time)) + c0
}

```


```{r plot-model-predictions}

# Check for data from one of those data.
# Public apixaban data?
rommel_fit = fit %>% 
  spread_draws(r_C[i]) %>% 
  mutate(r_C = 1000*r_C) %>% 
  mean_qi() %>% 
  bind_cols(rommel_data) %>% 
  rename(predictions=r_C)


ute_fit = fit %>% 
  spread_draws(u_C[i]) %>% 
  mutate(u_C = 1000*u_C) %>% 
  mean_qi() %>% 
  bind_cols(ute_data) %>% 
  rename(predictions=u_C)

rommel_fit %>% 
  bind_rows(ute_fit) %>% 
  ggplot(aes(log(yobs_ng_ml), log(predictions), ymin = log(.lower), ymax = log(.upper)))+
  geom_pointrange(size=0.1)+
  geom_abline(color='dark grey')+
  facet_wrap(~dataset, scale='free')+
  theme(aspect.ratio = 0.62)+
  labs(x='Log Concentration', y='Log Prediction')

ggsave('prediction_v_actual.png', path='figures')
knitr::plot_crop('figures/prediction_v_actual.png')

```

```{r}

fit %>% 
  spread_draws(mu_cl, s_cl, mu_tmax, s_t, mu_alpha, s_alpha, mu_F, b_F_amio) %>% 
  mutate(
    cl = exp(mu_cl),
    tmax = exp(mu_tmax),
    a = plogis(mu_alpha),
    ka = log(a)/(tmax*(a-1)),
    ke = ka*a
  ) %>% 
  select(cl, tmax, ka, ke, mu_F, b_F_amio) %>%  
  crossing(amio = seq(0, 1, 0.05)) %>% 
  mutate(f = plogis(mu_F + b_F_amio*amio),
         y = conc(tmax, 5, f, cl, ke, ka, 0)) %>% 
  ggplot(aes(amio, y))+
  stat_lineribbon(.width = c(0.5, 0.8, 0.95, 0.99), size=1)+
  scale_fill_brewer(palette ='YlGnBu')+
  scale_x_continuous(labels = function(x) 400*x)

```

# Discussion

In cases like apixaban, where real world data indicates previously published models may underestimate some aspect of pharmacokientics important to personalzing doses, investigators may consider constructing their own model for use in personalization as well as in exploration for novel effects.  In such cases making use of all available data, be the data from densley sampled highly controled studies or routinely collected samples from a personalized medicine clinic, is vital when running a large scale study is infeasible. In addition to tailoring the model to the population of interest, investigators may also be interested in examingin effects of covariates not previously studied.  These models hence play two roles: prediction and exploration.

Investigators may also be interested in detecting novel effects of new covariates (e.g. concomitant medications) not previously studied in clinical trials.  Where as previous studies use variable selection, we propose using sparse priors to detect large effects at the cost of bias in those effects towards the null.  In our simulation, we are able to detect both small and large at the expense of bias in the estimate.  This bias attenuates as sample size increases, as expected.  From the perspective of exploration, the bias is acceptable for two reasons.  First, no single study is intended to be an *experimentum crucis*.  Exploratory studies are, from our perspective, intended to generate hypotheses to be further examined in subsequent studies and populations. In these cases, the more salient factors are the magnitude of the estimate (is the effect "big" or "small", for some definition of those) and the sign of the effect (does the new covariate increase or decrease some quantity).  







# References